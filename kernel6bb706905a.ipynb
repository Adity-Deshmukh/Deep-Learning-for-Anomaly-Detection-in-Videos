{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"a=np.arange(68)\nnp.random.shuffle(a)\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#code for shuffling minibatches\ni=0\nj=0\nwhile i<68:\n    Y=[]\n    while j<min(i+10,68):\n        X=np.load(\"/kaggle/input/minibatches/minibatch%d.npz\" % (a[j]))\n        X=X['arr_0']\n        j=j+1\n        Y.append(X)\n    Y1=np.vstack(Y)\n    np.random.shuffle(Y1)\n    Y1=np.split(Y1,Y1.shape[0]/16)\n    for index,arr in enumerate(Y1):\n        np.savez(\"/kaggle/working/shuffled_minibatch%d.npz\"%(i+index),arr)\n    i=j\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras.layers import (Input, Activation,\n                                     BatchNormalization, Conv3D,\n                                     LeakyReLU, Conv3DTranspose)\nfrom tensorflow.keras.layers import MaxPool3D\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\n\n\ndef AutoEncoderModel():\n    # encoder\n    X_input = Input((16, 128, 128, 3))\n\n    X = Conv3D(32, 3, padding='same')(X_input)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n    # current shape is 8x64x64x32\n    X = Conv3D(48, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n    # current shape is 4x32x32x48\n    X = Conv3D(64, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n    # current shape is 2x16x16x64\n    X = Conv3D(64, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(X)\n    # current shape is 2x16x16x64\n    # decoder\n\n    X = Conv3DTranspose(48, 2, strides=(2, 2, 2), padding='valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 4x32x32x48\n    X = Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 8x64x64x32\n    X = Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 16x128x128x32\n    X = Conv3D(3, 3, strides=(1, 1, 1), padding='same')(X)\n    X = Activation('sigmoid')(X)\n    # current shape is 16x128x128x3\n\n    model = Model(inputs=X_input, outputs=X, name='AutoEncoderModel')\n    return model\n\n\ndef custom_loss(new, original):\n    reconstruction_error = K.mean(K.square(new-original))\n    return reconstruction_error\n\nautoEncoderModel = AutoEncoderModel()\nopt = keras.optimizers.Adam(lr=0.001)\nautoEncoderModel.compile(\n    loss=custom_loss, optimizer=opt, metrics=['accuracy'])\nprint(autoEncoderModel.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import Flatten,Dense\ndef create_discriminator_model():\n\n    X_input = Input((16, 128, 128, 3))\n\n    # not sure about the axis in batch norm\n    # do we also add dropout after batchnorm/pooling?\n\n    # Convolutional Layers\n    # changed the no of filters\n    X = Conv3D(filters=32, kernel_size=(2, 2, 2), padding=\"same\")(X_input)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(X)\n\n    X = Conv3D(filters=64, kernel_size=(2, 2, 2), padding=\"same\")(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(X)\n\n    X = Conv3D(filters=128, kernel_size=(2, 2, 2), padding=\"same\")(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(X)\n\n    X = Conv3D(filters=128, kernel_size=(2, 2, 2), padding=\"same\")(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(X)\n\n    # to add the 5th layer change the cap to 32 frames\n\n    # X=Conv3D(filters=256,kernel_size=(2,2,2),padding=\"same\")(X)\n    # X=BatchNormalization()(X)\n    # X=Activation('relu')(X)\n    # X=MaxPool3D(pool_size=(2,2,2),strides=(2,2,2))(X)\n\n    # Fully connected layers\n\n    X = Flatten()(X)\n\n    X = Dense(256, activation='relu')(X)\n    # add batch norm to dense layer\n    X = BatchNormalization()(X)\n    # activation done with loss fn\n    # for numerical stability\n    X = Dense(1, activation='sigmoid')(X)\n\n    model = Model(inputs=X_input, outputs=X, name=\"Discriminator\")\n\n    return model\n\ndiscriminator = create_discriminator_model()\nopt = keras.optimizers.Adam(lr=0.001)\nloss = BinaryCrossentropy()\ndiscriminator.compile(loss=loss,\n                      optimizer=opt,\n                      metrics=['accuracy'])\nprint(discriminator.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nclass GAN():\n    def __init__(self):\n        self.image_shape=(16,128,128,3)\n        opt=keras.optimizers.Adam(lr=0.001)\n        #Build and compile the discriminator\n        self.discriminator=create_discriminator_model()\n        self.discriminator.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n        #Build and compile the generator\n        self.generator=AutoEncoderModel()\n        self.generator.compile(loss=custom_loss,optimizer=opt,metrics=['accuracy'])\n\n        #the generator takes a video as input and generates a modified video\n        z=Input(shape=self.image_shape)\n        modified_vid=self.generator(z)\n        # For the combined model we will only train the generator\n        self.discriminator.trainable=False\n         # The valid takes generated images as input and determines validity\n        valid = self.discriminator(modified_vid)\n        # The combined model  (stacked generator and discriminator) takes\n        # video segment as input => generates modified video => determines validity\n        self.combined = Model(z, [valid,modified_vid])\n        # we need multiple losses as we need the normal loss function+reconstruction error\n        # not sure if this is the right way to implement it\n        #had to change the last 2 lines from the original code, not sure why\n        lossWeights = {\"Discriminator\": 1.0, \"AutoEncoderModel\": 0.1} #lossweights can be changed later\n        self.combined.compile(loss={'Discriminator':'binary_crossentropy','AutoEncoderModel':'mse'}, optimizer=opt,loss_weights=lossWeights)\n\n    def train(self,epochs,mini_batch_size):\n        #this function will need to be added later\n        for epoch in range(epochs):\n            for i in range(68):\n                # ---------------------\n                #  Train Discriminator\n                # ---------------------\n                minibatch=np.load('/kaggle/working/shuffled_minibatch%d.npz' %(i))\n                minibatch=minibatch['arr_0']\n                gen_vids=self.generator.predict(minibatch)\n                #might have to combine these to improve batch norm\n                d_loss_real=self.discriminator.train_on_batch(minibatch,np.ones((mini_batch_size,1)))\n                d_loss_fake=self.discriminator.train_on_batch(gen_vids,np.zeros((mini_batch_size,1)))\n                d_loss=0.5*np.add(d_loss_real,d_loss_fake)\n                # ---------------------\n                #  Train Generator\n                # ---------------------\n                # The generator wants the discriminator to label the generated samples as valid (ones)\n                valid_y = np.array([1] * mini_batch_size)\n                # Train the generator\n                g_loss = self.combined.train_on_batch(minibatch, {'Discriminator': valid_y,'AutoEncoderModel':K.cast(minibatch,'float32')})\n            # Plot the progress\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n        \ngan = GAN()\nprint(gan.combined.summary())\n# print(gan.discriminator.summary())\n# print(gan.generator.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.train(1,16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}